"""
æ•´åˆçš„é‡‘èç ”æŠ¥ç”Ÿæˆå™¨
åŒ…å«æ•°æ®é‡‡é›†ã€åˆ†æå’Œæ·±åº¦ç ”æŠ¥ç”Ÿæˆçš„å®Œæ•´æµç¨‹
- ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†ä¸åŸºç¡€åˆ†æ
- ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç ”æŠ¥ç”Ÿæˆä¸æ ¼å¼åŒ–è¾“å‡º
"""

import os
import glob
import time
import json
import yaml
import re
import shutil
import requests
from datetime import datetime
from dotenv import load_dotenv
import importlib
from urllib.parse import urlparse

# from data_analysis_agent import quick_analysis
from config.llm_config import LLMConfig
from utils.llm_helper import LLMHelper
from toolset.utils.get_shareholder_info import get_shareholder_info, get_table_content
from toolset.utils.get_financial_statements import get_all_financial_statements, save_financial_statements_to_csv
from toolset.utils.identify_competitors import identify_competitors_with_ai
from toolset.utils.get_stock_intro import get_stock_intro, save_stock_intro_to_txt
from duckduckgo_search import DDGS
from toolset.utils.search_engine import SearchEngine
# from data_analysis_agent import DataAnalysisAgent
from toolset.utils.create_session_dir import create_session_output_dir
from toolset.utils.code_executor import CodeExecutor
from toolset.utils.extract_code import extract_code_from_response
from toolset.utils.format_execution_result import format_execution_result
from toolset.utils.code_executor import CodeExecutor
from typing import List, Dict, Any, Optional
from prompts.planner.prompts import data_analysis_system_prompt, final_report_system_prompt,final_report_system_prompt_absolute


class IntegratedResearchReportGenerator:
    """æ•´åˆçš„ç ”æŠ¥ç”Ÿæˆå™¨ç±»"""    
    
    def __init__(self, target_company="å•†æ±¤ç§‘æŠ€", target_company_code="00020", target_company_market="HK", search_engine="ddg"):
        # ç¯å¢ƒå˜é‡ä¸å…¨å±€é…ç½®
        load_dotenv()
        self.api_key = os.getenv("OPENAI_API_KEY", "")
        self.base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.model = os.getenv("OPENAI_MODEL", "gpt-4")
        # æ‰“å°æ¨¡å‹
        print(f"ğŸ”§ ä½¿ç”¨çš„æ¨¡å‹: {self.model}")
        self.target_company = target_company
        self.target_company_code = target_company_code
        self.target_company_market = target_company_market
        
        # æœç´¢å¼•æ“é…ç½®
        self.search_engine = SearchEngine(search_engine)
        print(f"ğŸ” æœç´¢å¼•æ“å·²é…ç½®ä¸º: {search_engine.upper()}")
        
        # ç›®å½•é…ç½®
        self.data_dir = "./download_financial_statement_files"
        self.company_info_dir = "./company_info"
        self.industry_info_dir = "./industry_info"
        self.absolute_path = False
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        for dir_path in [self.data_dir, self.company_info_dir, self.industry_info_dir]:
            os.makedirs(dir_path, exist_ok=True)
        
        # LLMé…ç½®
        self.llm_config = LLMConfig(
            api_key=self.api_key,
            base_url=self.base_url,
            model=self.model,
            temperature=0.7,
            max_tokens=8192,
        )
        self.llm = LLMHelper(self.llm_config)
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.analysis_results = {}
    
    def stage1_data_collection(self):
        """ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†ä¸åŸºç¡€åˆ†æ"""
        print("\n" + "="*80)
        print("ğŸš€ å¼€å§‹ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†ä¸åŸºç¡€åˆ†æ")
        print("="*80)
        
        # 1. è·å–ç«äº‰å¯¹æ‰‹åˆ—è¡¨
        print("ğŸ” è¯†åˆ«ç«äº‰å¯¹æ‰‹...")
        other_companies = identify_competitors_with_ai(
            api_key=self.api_key,
            base_url=self.base_url,
            model_name=self.model,
            company_name=self.target_company
        )
        listed_companies = [company for company in other_companies if company.get('market') != "æœªä¸Šå¸‚"]
        
        # 2. è·å–ç›®æ ‡å…¬å¸è´¢åŠ¡æ•°æ®
        print(f"\nğŸ“Š è·å–ç›®æ ‡å…¬å¸ {self.target_company} çš„è´¢åŠ¡æ•°æ®...")
        target_financials = get_all_financial_statements(
            stock_code=self.target_company_code,
            market=self.target_company_market,
            period="å¹´åº¦",
            verbose=False
        )
        save_financial_statements_to_csv(
            financial_statements=target_financials,
            stock_code=self.target_company_code,
            market=self.target_company_market,
            company_name=self.target_company,
            period="å¹´åº¦",
            save_dir=self.data_dir
        )
        
        # 3. è·å–ç«äº‰å¯¹æ‰‹çš„è´¢åŠ¡æ•°æ®
        print("\nğŸ“Š è·å–ç«äº‰å¯¹æ‰‹çš„è´¢åŠ¡æ•°æ®...")
        competitors_financials = {}
        for company in listed_companies:
            company_name = company.get('name', '')
            company_code = company.get('code', '')
            market_str = company.get('market', '')
            
            if "A" in market_str:
                market = "A"
                if not (company_code.startswith('SH') or company_code.startswith('SZ')):
                    if company_code.startswith('6'):
                        company_code = f"SH{company_code}"
                    else:
                        company_code = f"SZ{company_code}"
            elif "æ¸¯" in market_str:
                market = "HK"
            
            print(f"  è·å– {company_name}({market}:{company_code}) çš„è´¢åŠ¡æ•°æ®")
            try:
                company_financials = get_all_financial_statements(
                    stock_code=company_code,
                    market=market,
                    period="å¹´åº¦",
                    verbose=False
                )
                save_financial_statements_to_csv(
                    financial_statements=company_financials,
                    stock_code=company_code,
                    market=market,
                    company_name=company_name,
                    period="å¹´åº¦",
                    save_dir=self.data_dir
                )
                competitors_financials[company_name] = company_financials
                time.sleep(2)
            except Exception as e:
                print(f"  è·å– {company_name} è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
        
        # 4. è·å–å…¬å¸åŸºç¡€ä¿¡æ¯
        print("\nğŸ¢ è·å–å…¬å¸åŸºç¡€ä¿¡æ¯...")
        all_base_info_targets = [(self.target_company, self.target_company_code, self.target_company_market)]
        
        for company in listed_companies:
            company_name = company.get('name', '')
            company_code = company.get('code', '')
            market_str = company.get('market', '')
            if "A" in market_str:
                market = "A"
                if not (company_code.startswith('SH') or company_code.startswith('SZ')):
                    if company_code.startswith('6'):
                        company_code = f"SH{company_code}"
                    else:
                        company_code = f"SZ{company_code}"
            elif "æ¸¯" in market_str:
                market = "HK"
            all_base_info_targets.append((company_name, company_code, market))
        
        # æ·»åŠ ç‰¹å®šå…¬å¸å¦‚ç™¾åº¦
        all_base_info_targets.append(("ç™¾åº¦", "09888", "HK"))
        
        for company_name, company_code, market in all_base_info_targets:
            print(f"  è·å– {company_name}({market}:{company_code}) çš„åŸºç¡€ä¿¡æ¯")
            # Ensure market is properly typed as literal
            market_literal = "A" if market == "A" else "HK"
            company_info = get_stock_intro(company_code, market=market_literal)
            if company_info:
                save_path = os.path.join(self.company_info_dir, f"{company_name}_{market}_{company_code}_info.txt")
                save_stock_intro_to_txt(company_code, market_literal, save_path)
                print(f"    ä¿¡æ¯å·²ä¿å­˜åˆ°: {save_path}")
            else:
                print(f"    æœªèƒ½è·å–åˆ° {company_name} çš„åŸºç¡€ä¿¡æ¯")
            time.sleep(1)
        
        # 5. æœç´¢è¡Œä¸šä¿¡æ¯
        print("\nğŸ” æœç´¢è¡Œä¸šä¿¡æ¯...")
        all_search_results = {}
          # æœç´¢ç›®æ ‡å…¬å¸è¡Œä¸šä¿¡æ¯
        target_search_keywords = f"{self.target_company} è¡Œä¸šåœ°ä½ å¸‚åœºä»½é¢ ç«äº‰åˆ†æ ä¸šåŠ¡æ¨¡å¼"
        print(f"  æ­£åœ¨æœç´¢: {target_search_keywords}")
        # è¿›è¡Œç›®æ ‡å…¬å¸æœç´¢
        target_results = self.search_engine.search(target_search_keywords, 10)
        all_search_results[self.target_company] = target_results

        # æœç´¢ç«äº‰å¯¹æ‰‹è¡Œä¸šä¿¡æ¯
        for company in listed_companies:
            company_name = company.get('name')
            search_keywords = f"{company_name} è¡Œä¸šåœ°ä½ å¸‚åœºä»½é¢ ä¸šåŠ¡æ¨¡å¼ å‘å±•æˆ˜ç•¥"
            print(f"  æ­£åœ¨æœç´¢: {search_keywords}")
            competitor_results = self.search_engine.search(search_keywords, 10)
            all_search_results[company_name] = competitor_results
            # å¢åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(self.search_engine.delay * 2)
        
        # ä¿å­˜æœç´¢ç»“æœ
        search_results_file = os.path.join(self.industry_info_dir, "all_search_results.json")
        with open(search_results_file, 'w', encoding='utf-8') as f:
            json.dump(all_search_results, f, ensure_ascii=False, indent=2)
        
        # 6. è¿è¡Œè´¢åŠ¡åˆ†æ
        print("\nğŸ“ˆ è¿è¡Œè´¢åŠ¡åˆ†æ...")
        
        # å•å…¬å¸åˆ†æ
        results = self.analyze_companies_in_directory(self.data_dir, self.llm_config)
        
        # ä¸¤ä¸¤å¯¹æ¯”åˆ†æ
        comparison_results = self.run_comparison_analysis(
            self.data_dir, self.target_company, self.llm_config
        )
        
        # åˆå¹¶æ‰€æœ‰æŠ¥å‘Š
        merged_results = self.merge_reports(results, comparison_results)
        
        # å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æ
        sensetime_files = self.get_sensetime_files(self.data_dir)
        sensetime_valuation_report = None
        if sensetime_files:
            sensetime_valuation_report = self.analyze_sensetime_valuation(sensetime_files, self.llm_config)
        
        # 7. æ•´ç†æ‰€æœ‰åˆ†æç»“æœ
        print("\nğŸ“‹ æ•´ç†åˆ†æç»“æœ...")
        
        # æ•´ç†å…¬å¸ä¿¡æ¯
        company_infos = self.get_company_infos(self.company_info_dir)
        company_infos = self.llm.call(
            f"è¯·æ•´ç†ä»¥ä¸‹å…¬å¸ä¿¡æ¯å†…å®¹ï¼Œç¡®ä¿æ ¼å¼æ¸…æ™°æ˜“è¯»ï¼Œå¹¶ä¿ç•™å…³é”®ä¿¡æ¯ï¼š\n{company_infos}",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¬å¸ä¿¡æ¯æ•´ç†å¸ˆã€‚",
            max_tokens=8192,
            temperature=0.5
        )
        
        # æ•´ç†è‚¡æƒä¿¡æ¯
        info = get_shareholder_info()
        shangtang_shareholder_info = info.get("tables", [])
        table_content = get_table_content(shangtang_shareholder_info)
        shareholder_analysis = self.llm.call(
            "è¯·åˆ†æä»¥ä¸‹è‚¡ä¸œä¿¡æ¯è¡¨æ ¼å†…å®¹ï¼š\n" + table_content,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ä¸œä¿¡æ¯åˆ†æå¸ˆã€‚",
            max_tokens=8192,
            temperature=0.5
        )
        
        # æ•´ç†è¡Œä¸šä¿¡æ¯æœç´¢ç»“æœ
        with open(search_results_file, 'r', encoding='utf-8') as f:
            all_search_results = json.load(f)
        search_res = ""
        for company, results in all_search_results.items():
            search_res += f"ã€{company}æœç´¢ä¿¡æ¯å¼€å§‹ã€‘\n"
            for result in results:
                search_res += f"æ ‡é¢˜: {result.get('title', 'æ— æ ‡é¢˜')}\n"
                search_res += f"é“¾æ¥: {result.get('href', 'æ— é“¾æ¥')}\n"
                search_res += f"æ‘˜è¦: {result.get('body', 'æ— æ‘˜è¦')}\n"
                search_res += "----\n"
            search_res += f"ã€{company}æœç´¢ä¿¡æ¯ç»“æŸã€‘\n\n"
        
        # ä¿å­˜é˜¶æ®µä¸€ç»“æœ
        formatted_report = self.format_final_reports(merged_results)
        
        # ç»Ÿä¸€ä¿å­˜ä¸ºmarkdown
        md_output_file = f"è´¢åŠ¡ç ”æŠ¥æ±‡æ€»_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(md_output_file, 'w', encoding='utf-8') as f:
            f.write(f"# å…¬å¸åŸºç¡€ä¿¡æ¯\n\n## æ•´ç†åå…¬å¸ä¿¡æ¯\n\n{company_infos}\n\n")
            f.write(f"# è‚¡æƒä¿¡æ¯åˆ†æ\n\n{shareholder_analysis}\n\n")
            f.write(f"# è¡Œä¸šä¿¡æ¯æœç´¢ç»“æœ\n\n{search_res}\n\n")
            f.write(f"# è´¢åŠ¡æ•°æ®åˆ†æä¸ä¸¤ä¸¤å¯¹æ¯”\n\n{formatted_report}\n\n")
            if sensetime_valuation_report and isinstance(sensetime_valuation_report, dict):
                f.write(f"# å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æ\n\n{sensetime_valuation_report.get('final_report', 'æœªç”ŸæˆæŠ¥å‘Š')}\n\n")
        
        print(f"\nâœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼åŸºç¡€åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {md_output_file}")
        
        # å­˜å‚¨ç»“æœä¾›ç¬¬äºŒé˜¶æ®µä½¿ç”¨
        self.analysis_results = {
            'md_file': md_output_file,
            'company_infos': company_infos,
            'shareholder_analysis': shareholder_analysis,
            'search_res': search_res,
            'formatted_report': formatted_report,
            'sensetime_valuation_report': sensetime_valuation_report
        }
        
        return md_output_file
    
    def stage2_deep_report_generation(self, md_file_path):
        """ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç ”æŠ¥ç”Ÿæˆ"""
        print("\n" + "="*80)
        print("ğŸš€ å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç ”æŠ¥ç”Ÿæˆ")
        print("="*80)
        
        # å¤„ç†å›¾ç‰‡è·¯å¾„
        print("ğŸ–¼ï¸ å¤„ç†å›¾ç‰‡è·¯å¾„...")
        new_md_path = md_file_path.replace('.md', '_images.md')
        images_dir = os.path.join(os.path.dirname(md_file_path), 'images')
        self.extract_images_from_markdown(md_file_path, images_dir, new_md_path)
        
        # åŠ è½½æŠ¥å‘Šå†…å®¹
        report_content = self.load_report_content(new_md_path)
        background = self.get_background()
        
        # ç”Ÿæˆå¤§çº²
        print("\nğŸ“‹ ç”ŸæˆæŠ¥å‘Šå¤§çº²...")
        parts = self.generate_outline(self.llm, background, report_content)
        
        # åˆ†æ®µç”Ÿæˆæ·±åº¦ç ”æŠ¥
        print("\nâœï¸ å¼€å§‹åˆ†æ®µç”Ÿæˆæ·±åº¦ç ”æŠ¥...")
        full_report = ['# å•†æ±¤ç§‘æŠ€å…¬å¸ç ”æŠ¥\n']
        prev_content = ''
        
        for idx, part in enumerate(parts):
            part_title = part.get('part_title', f'éƒ¨åˆ†{idx+1}')
            print(f"\n  æ­£åœ¨ç”Ÿæˆï¼š{part_title}")
            is_last = (idx == len(parts) - 1)
            section_text = self.generate_section(
                self.llm, part_title, prev_content, background, report_content, is_last
            )
            full_report.append(section_text)
            print(f"  âœ… å·²å®Œæˆï¼š{part_title}")
            prev_content = '\n'.join(full_report)
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        final_report = '\n\n'.join(full_report)
        output_file = f"æ·±åº¦è´¢åŠ¡ç ”æŠ¥åˆ†æ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        self.save_markdown(final_report, output_file)
        
        # æ ¼å¼åŒ–å’Œè½¬æ¢
        print("\nğŸ¨ æ ¼å¼åŒ–æŠ¥å‘Š...")
        self.format_markdown(output_file)
        
        print("\nğŸ“„ è½¬æ¢ä¸ºWordæ–‡æ¡£...")
        self.convert_to_docx(output_file)
        
        print(f"\nâœ… ç¬¬äºŒé˜¶æ®µå®Œæˆï¼æ·±åº¦ç ”æŠ¥å·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        print("\n" + "="*100)
        print("ğŸ¯ å¯åŠ¨æ•´åˆçš„é‡‘èç ”æŠ¥ç”Ÿæˆæµç¨‹")
        print("="*100)
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†ä¸åŸºç¡€åˆ†æ
        md_file = self.stage1_data_collection()
        
        # ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç ”æŠ¥ç”Ÿæˆ
        final_report = self.stage2_deep_report_generation(md_file)
        
        print("\n" + "="*100)
        print("ğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæ¯•ï¼")
        print(f"ğŸ“Š åŸºç¡€åˆ†ææŠ¥å‘Š: {md_file}")
        print(f"ğŸ“‹ æ·±åº¦ç ”æŠ¥: {final_report}")
        print("="*100)
        
        return md_file, final_report

    # ========== è¾…åŠ©æ–¹æ³•ï¼ˆä»åŸå§‹è„šæœ¬ç§»æ¤ï¼‰ ==========
    # ä¾¿æ·å‡½æ•°
    # def create_agent(self, llm_config=None, output_dir="outputs", max_rounds=30,absolute_path=False):
    #     """
    #     åˆ›å»ºä¸€ä¸ªæ•°æ®åˆ†ææ™ºèƒ½ä½“å®ä¾‹
        
    #     Args:
    #         config: LLMé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    #         output_dir: è¾“å‡ºç›®å½•
    #         max_rounds: æœ€å¤§åˆ†æè½®æ•°
    #         session_dir: æŒ‡å®šä¼šè¯ç›®å½•ï¼ˆå¯é€‰ï¼Œæ­¤å‚æ•°æš‚ä¸æ”¯æŒï¼‰
            
    #     Returns:
    #         DataAnalysisAgent: æ™ºèƒ½ä½“å®ä¾‹
    #     """
    #     if llm_config is None:
    #         llm_config = LLMConfig()
    #     return DataAnalysisAgent(llm_config=llm_config, output_dir=output_dir, max_rounds=max_rounds,absolute_path=absolute_path)

    def _process_response(self, response: str) -> Dict[str, Any]:
        """
        ç»Ÿä¸€å¤„ç†LLMå“åº”ï¼Œåˆ¤æ–­è¡ŒåŠ¨ç±»å‹å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ
        
        Args:
            response: LLMçš„å“åº”å†…å®¹
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            yaml_data = self.llm.parse_yaml_response(response)
            action = yaml_data.get('action', 'generate_code')
            
            print(f"ğŸ¯ æ£€æµ‹åˆ°åŠ¨ä½œ: {action}")
            
            if action == 'analysis_complete':
                return self._handle_analysis_complete(response, yaml_data)
            elif action == 'collect_figures':
                return self._handle_collect_figures(response, yaml_data)
            elif action == 'generate_code':
                return self._handle_generate_code(response, yaml_data)
            else:
                print(f"âš ï¸ æœªçŸ¥åŠ¨ä½œç±»å‹: {action}ï¼ŒæŒ‰generate_codeå¤„ç†")
                return self._handle_generate_code(response, yaml_data)
                
        except Exception as e:
            print(f"âš ï¸ è§£æå“åº”å¤±è´¥: {str(e)}ï¼ŒæŒ‰generate_codeå¤„ç†")
            return self._handle_generate_code(response, {})
    
    def _handle_analysis_complete(self, response: str, yaml_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†åˆ†æå®ŒæˆåŠ¨ä½œ"""
        print("âœ… åˆ†æä»»åŠ¡å®Œæˆ")
        final_report = yaml_data.get('final_report', 'åˆ†æå®Œæˆï¼Œæ— æœ€ç»ˆæŠ¥å‘Š')
        return {
            'action': 'analysis_complete',
            'final_report': final_report,
            'response': response,
            'continue': False
        }
    
    def _handle_collect_figures(self, response: str, yaml_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å›¾ç‰‡æ”¶é›†åŠ¨ä½œ"""
        print("ğŸ“Š å¼€å§‹æ”¶é›†å›¾ç‰‡")
        figures_to_collect = yaml_data.get('figures_to_collect', [])
        
        collected_figures = []
        
        for figure_info in figures_to_collect:
            figure_number = figure_info.get('figure_number')
            filename = figure_info.get('filename', f'figure_{figure_number}.png')
            file_path = figure_info.get('file_path', '')  # è·å–å…·ä½“çš„æ–‡ä»¶è·¯å¾„
            description = figure_info.get('description', '')
            analysis = figure_info.get('analysis', '')
            
            print(f"ğŸ“ˆ æ”¶é›†å›¾ç‰‡ {figure_number}: {filename}")
            print(f"   ğŸ“‚ è·¯å¾„: {file_path}")
            print(f"   ğŸ“ æè¿°: {description}")
            print(f"   ğŸ” åˆ†æ: {analysis}")
            
            # åªä¿ç•™çœŸå®å­˜åœ¨çš„å›¾ç‰‡
            if file_path and os.path.exists(file_path):
                print(f"   âœ… æ–‡ä»¶å­˜åœ¨: {file_path}")
                collected_figures.append({
                    'figure_number': figure_number,
                    'filename': filename,
                    'file_path': file_path,
                    'description': description,
                    'analysis': analysis
                })
            elif file_path:
                print(f"   âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}ï¼Œå·²è·³è¿‡è¯¥å›¾ç‰‡")
            else:
                print(f"   âš ï¸ æœªæä¾›æ–‡ä»¶è·¯å¾„ï¼Œå·²è·³è¿‡è¯¥å›¾ç‰‡")
        
        
        return {
            'action': 'collect_figures',
            'collected_figures': collected_figures,
            'response': response,  # response ä»ç„¶åŸæ ·ä¿ç•™ï¼Œè‹¥éœ€å½»åº•å‡€åŒ–å¯è¿›ä¸€æ­¥å¤„ç†
            'continue': True
        }

    def _handle_generate_code(self, response: str, yaml_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†ä»£ç ç”Ÿæˆå’Œæ‰§è¡ŒåŠ¨ä½œ"""
        code = yaml_data.get('code', '')
        if not code:
            code = extract_code_from_response(response)
        if code:
            print(f"ğŸ”§ æ‰§è¡Œä»£ç :\n{code}")
            print("-" * 40)
            result = self.executor.execute_code(code)
            feedback = format_execution_result(result)
            print(f"ğŸ“‹ æ‰§è¡Œåé¦ˆ:\n{feedback}")
            # æ£€æŸ¥ä»£ç æ‰§è¡Œç»“æœä¸­æ˜¯å¦æœ‰å›¾ç‰‡ç”Ÿæˆä½†æ–‡ä»¶ä¸å­˜åœ¨çš„æƒ…å†µ
            # å‡è®¾å›¾ç‰‡ä¿å­˜è·¯å¾„ä¼šåœ¨ result['output'] æˆ– result['figures'] é‡Œä½“ç°
            # å¦‚æœæ£€æµ‹åˆ°å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå»ºè®®ç”¨æˆ·é‡æ–°åˆ†æ
            missing_figures = []
            output = result.get('output', '')
            # ç®€å•æ­£åˆ™æˆ–å­—ç¬¦ä¸²æŸ¥æ‰¾å›¾ç‰‡è·¯å¾„å¹¶åˆ¤æ–­æ˜¯å¦å­˜åœ¨
            import re
            img_paths = re.findall(r'(?:[\w./\\-]+\.(?:png|jpg|jpeg|svg))', str(output))
            for img_path in img_paths:
                if not os.path.isabs(img_path):
                    abs_path = os.path.join(self.session_output_dir, img_path)
                else:
                    abs_path = img_path
                if not os.path.exists(abs_path):
                    missing_figures.append(img_path)
            if missing_figures:
                feedback += f"\nâš ï¸ æ£€æµ‹åˆ°ä»¥ä¸‹å›¾ç‰‡æœªç”ŸæˆæˆåŠŸ: {missing_figures}\nå»ºè®®é‡æ–°åˆ†ææœ¬è½®æˆ–ä¿®æ­£ä»£ç åå†è¯•ã€‚"
                # å¯ä»¥åœ¨è¿™é‡Œè¿”å›ä¸€ä¸ªç‰¹æ®Šæ ‡å¿—ï¼Œä¾› analyze ä¸»æµç¨‹åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å¯åˆ†æ
                return {
                    'action': 'generate_code',
                    'code': code,
                    'result': result,
                    'feedback': feedback,
                    'response': response,
                    'continue': False,  # ç»ˆæ­¢æœ¬è½®åˆ†æ
                    'need_restart': True,
                    'missing_figures': missing_figures
                }
            return {
                'action': 'generate_code',
                'code': code,
                'result': result,
                'feedback': feedback,
                'response': response,
                'continue': True
            }
        else:
            print("âš ï¸ æœªä»å“åº”ä¸­æå–åˆ°å¯æ‰§è¡Œä»£ç ï¼Œè¦æ±‚LLMé‡æ–°ç”Ÿæˆ")
            return {
                'action': 'invalid_response',
                'error': 'å“åº”ä¸­ç¼ºå°‘å¯æ‰§è¡Œä»£ç ',
                'response': response,
                'continue': True
            }

    def _build_conversation_prompt(self) -> str:
        """æ„å»ºå¯¹è¯æç¤ºè¯"""
        prompt_parts = []
        
        for msg in self.conversation_history:
            role = msg['role']
            content = msg['content']
            if role == 'user':
                prompt_parts.append(f"ç”¨æˆ·: {content}")
            else:
                prompt_parts.append(f"åŠ©æ‰‹: {content}")
        
        return "\n\n".join(prompt_parts)
    
    def _generate_final_report(self, analysis_results:List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
        # æ”¶é›†æ‰€æœ‰ç”Ÿæˆçš„å›¾ç‰‡ä¿¡æ¯
        all_figures = []
        for result in analysis_results:
            if result.get('action') == 'collect_figures':
                all_figures.extend(result.get('collected_figures', []))
        
        print(f"\nğŸ“Š å¼€å§‹ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š...")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.session_output_dir}")
        print(f"ğŸ”¢ æ€»è½®æ•°: {self.current_round}")
        print(f"ğŸ“ˆ æ”¶é›†å›¾ç‰‡: {len(all_figures)} ä¸ª")
        
        # æ„å»ºç”¨äºç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šçš„æç¤ºè¯
        final_report_prompt = self._build_final_report_prompt(all_figures, analysis_results)
        
        # è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        response = self.llm.call(
            prompt=final_report_prompt,
            system_prompt="ä½ å°†ä¼šæ¥æ”¶åˆ°ä¸€ä¸ªæ•°æ®åˆ†æä»»åŠ¡çš„æœ€ç»ˆæŠ¥å‘Šè¯·æ±‚ï¼Œè¯·æ ¹æ®æä¾›çš„åˆ†æç»“æœå’Œå›¾ç‰‡ä¿¡æ¯ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Šã€‚",
            max_tokens=8192  
        )
        
        # è§£æå“åº”ï¼Œæå–æœ€ç»ˆæŠ¥å‘Š
        try:
            yaml_data = self.llm.parse_yaml_response(response)
            if yaml_data.get('action') == 'analysis_complete':
                final_report_content = yaml_data.get('final_report', 'æŠ¥å‘Šç”Ÿæˆå¤±è´¥')
            else:
                final_report_content = "LLMæœªè¿”å›analysis_completeåŠ¨ä½œï¼ŒæŠ¥å‘Šç”Ÿæˆå¤±è´¥"
        except:
            # å¦‚æœè§£æå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨å“åº”å†…å®¹
            final_report_content = response
        
        print("âœ… æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        # æ‰‹åŠ¨æ·»åŠ é™„ä»¶æ¸…å•åˆ°æŠ¥å‘Šæœ«å°¾
        if all_figures:
            appendix_section = "\n\n## é™„ä»¶æ¸…å•\n\n"
            appendix_section += "æœ¬æŠ¥å‘ŠåŒ…å«ä»¥ä¸‹å›¾ç‰‡é™„ä»¶ï¼š\n\n"
            
            for i, figure in enumerate(all_figures, 1):
                filename = figure.get('filename', 'æœªçŸ¥æ–‡ä»¶å')
                description = figure.get('description', 'æ— æè¿°')
                analysis = figure.get('analysis', 'æ— åˆ†æ')
                file_path = figure.get('file_path', '')
                
                appendix_section += f"{i}. **{filename}**\n"
                appendix_section += f"   - æè¿°ï¼š{description}\n"
                appendix_section += f"   - ç»†èŠ‚åˆ†æï¼š{analysis}\n"
                if self.absolute_path:
                    appendix_section += f"   - æ–‡ä»¶è·¯å¾„ï¼š{file_path}\n"
                else:
                    appendix_section += f"   - æ–‡ä»¶è·¯å¾„ï¼š./{filename}\n"
                appendix_section += "\n"
            
            # å°†é™„ä»¶æ¸…å•æ·»åŠ åˆ°æŠ¥å‘Šå†…å®¹æœ«å°¾
            final_report_content += appendix_section
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file_path = os.path.join(self.session_output_dir, "æœ€ç»ˆåˆ†ææŠ¥å‘Š.md")
        try:
            with open(report_file_path, 'w', encoding='utf-8') as f:
                f.write(final_report_content)
            print(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file_path}")
            if all_figures:
                print(f"ğŸ“ å·²æ·»åŠ  {len(all_figures)} ä¸ªå›¾ç‰‡çš„é™„ä»¶æ¸…å•")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # è¿”å›å®Œæ•´çš„åˆ†æç»“æœ
        return {
            'session_output_dir': self.session_output_dir,
            'total_rounds': self.current_round,
            'analysis_results': analysis_results,
            'collected_figures': all_figures,
            'conversation_history': self.conversation_history,
            'final_report': final_report_content,
            'report_file_path': report_file_path       
            }

    def _build_final_report_prompt(self, all_figures: List[Dict[str, Any]], analysis_results:List[Dict[str, Any]]) -> str:
        """æ„å»ºç”¨äºç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šçš„æç¤ºè¯"""
        
        # æ„å»ºå›¾ç‰‡ä¿¡æ¯æ‘˜è¦ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
        figures_summary = ""
        if all_figures:
            figures_summary = "\nç”Ÿæˆçš„å›¾ç‰‡åŠåˆ†æ:\n"
            for i, figure in enumerate(all_figures, 1):
                filename = figure.get('filename', 'æœªçŸ¥æ–‡ä»¶å')
                # ä½¿ç”¨ç›¸å¯¹è·¯å¾„æ ¼å¼ï¼Œé€‚åˆåœ¨æŠ¥å‘Šä¸­å¼•ç”¨
                relative_path = f"./{filename}"
                figures_summary += f"{i}. {filename}\n"
                figures_summary += f"   è·¯å¾„: {relative_path}\n"
                figures_summary += f"   æè¿°: {figure.get('description', 'æ— æè¿°')}\n"
                figures_summary += f"   åˆ†æ: {figure.get('analysis', 'æ— åˆ†æ')}\n\n"
        else:
            figures_summary = "\næœ¬æ¬¡åˆ†ææœªç”Ÿæˆå›¾ç‰‡ã€‚\n"
        
        # æ„å»ºä»£ç æ‰§è¡Œç»“æœæ‘˜è¦ï¼ˆä»…åŒ…å«æˆåŠŸæ‰§è¡Œçš„ä»£ç å—ï¼‰
        code_results_summary = ""
        success_code_count = 0
        for result in analysis_results:
            if result.get('action') != 'collect_figures' and result.get('code'):
                exec_result = result.get('result', {})
                if exec_result.get('success'):
                    success_code_count += 1
                    code_results_summary += f"ä»£ç å— {success_code_count}: æ‰§è¡ŒæˆåŠŸ\n"
                    if exec_result.get('output'):
                        code_results_summary += f"è¾“å‡º: {exec_result.get('output')[:]}\n\n"

        
        # ä½¿ç”¨ prompts.py ä¸­çš„ç»Ÿä¸€æç¤ºè¯æ¨¡æ¿ï¼Œå¹¶æ·»åŠ ç›¸å¯¹è·¯å¾„ä½¿ç”¨è¯´æ˜
        pre_prompt = final_report_system_prompt_absolute if self.absolute_path else final_report_system_prompt
        prompt = pre_prompt.format(
            current_round=self.current_round,
            session_output_dir=self.session_output_dir,
            figures_summary=figures_summary,
            code_results_summary=code_results_summary
        )
        
        return prompt


    def analyze(self, user_input: str, files:Optional[List[str]] = None, max_rounds=6) -> Dict[str, Any]:
        """
        å¼€å§‹åˆ†ææµç¨‹
        
        Args:
            user_input: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€éœ€æ±‚
            files: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        # é‡ç½®çŠ¶æ€
        self.conversation_history = []
        self.analysis_results = []
        self.current_round = 0
        base_output_dir = "outputs"

        if files is None:
            files = []
        
        # åˆ›å»ºæœ¬æ¬¡åˆ†æçš„ä¸“ç”¨è¾“å‡ºç›®å½•
        self.session_output_dir = create_session_output_dir(base_output_dir, user_input)
        
        # åˆå§‹åŒ–ä»£ç æ‰§è¡Œå™¨ï¼Œä½¿ç”¨ä¼šè¯ç›®å½•
        self.executor = CodeExecutor(self.session_output_dir)
        
        # è®¾ç½®ä¼šè¯ç›®å½•å˜é‡åˆ°æ‰§è¡Œç¯å¢ƒä¸­
        self.executor.set_variable('session_output_dir', self.session_output_dir)
        
        # æ„å»ºåˆå§‹prompt
        initial_prompt = f"""ç”¨æˆ·éœ€æ±‚: {user_input}"""
        if files:
            initial_prompt += f"\næ•°æ®æ–‡ä»¶: {', '.join(files)}"
        
        print(f"ğŸš€ å¼€å§‹æ•°æ®åˆ†æä»»åŠ¡")
        print(f"ğŸ“ ç”¨æˆ·éœ€æ±‚: {user_input}")
        if files:
            print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {', '.join(files)}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.session_output_dir}")
        print(f"ğŸ”¢ æœ€å¤§è½®æ•°: {max_rounds}")
        print("=" * 60)
          # æ·»åŠ åˆ°å¯¹è¯å†å²
        self.conversation_history.append({
            'role': 'user',
            'content': initial_prompt
        })
        
        while self.current_round < max_rounds:
            self.current_round += 1
            print(f"\nğŸ”„ ç¬¬ {self.current_round} è½®åˆ†æ")
              # è°ƒç”¨LLMç”Ÿæˆå“åº”
            try:                
                # è·å–å½“å‰æ‰§è¡Œç¯å¢ƒçš„å˜é‡ä¿¡æ¯
                notebook_variables = self.executor.get_environment_info()
                
                # æ ¼å¼åŒ–ç³»ç»Ÿæç¤ºè¯ï¼Œå¡«å…¥åŠ¨æ€çš„notebookå˜é‡ä¿¡æ¯
                formatted_system_prompt = data_analysis_system_prompt.format(
                    notebook_variables=notebook_variables
                )
                
                response = self.llm.call(
                    prompt=self._build_conversation_prompt(),
                    system_prompt=formatted_system_prompt
                )
                
                print(f"ğŸ¤– åŠ©æ‰‹å“åº”:\n{response}")
                
                # ä½¿ç”¨ç»Ÿä¸€çš„å“åº”å¤„ç†æ–¹æ³•
                process_result = self._process_response(response)
                
                # æ ¹æ®å¤„ç†ç»“æœå†³å®šæ˜¯å¦ç»§ç»­
                if not process_result.get('continue', True):
                    print(f"\nâœ… åˆ†æå®Œæˆï¼")
                    break
                
                # æ·»åŠ åˆ°å¯¹è¯å†å²
                self.conversation_history.append({
                    'role': 'assistant',
                    'content': response
                })
                
                # æ ¹æ®åŠ¨ä½œç±»å‹æ·»åŠ ä¸åŒçš„åé¦ˆ
                if process_result['action'] == 'generate_code':
                    feedback = process_result.get('feedback', '')
                    self.conversation_history.append({
                        'role': 'user',
                        'content': f"ä»£ç æ‰§è¡Œåé¦ˆ:\n{feedback}"
                    })
                    
                    # è®°å½•åˆ†æç»“æœ
                    self.analysis_results.append({
                        'round': self.current_round,
                        'code': process_result.get('code', ''),
                        'result': process_result.get('result', {}),
                        'response': response
                    })                
                elif process_result['action'] == 'collect_figures':
                    # è®°å½•å›¾ç‰‡æ”¶é›†ç»“æœ
                    collected_figures = process_result.get('collected_figures', [])
                    filtered_figures_to_collect = process_result.get('filtered_figures_to_collect', [])
                    feedback = f"å·²æ”¶é›† {len(collected_figures)} ä¸ªå›¾ç‰‡åŠå…¶åˆ†æ"
                    self.conversation_history.append({
                        'role': 'user', 
                        'content': f"å›¾ç‰‡æ”¶é›†åé¦ˆ:\n{feedback}\nè¯·ç»§ç»­ä¸‹ä¸€æ­¥åˆ†æã€‚"
                    })
                    # åªè®°å½•è¿‡æ»¤åçš„å›¾ç‰‡è®°å¿†
                    self.analysis_results.append({
                        'round': self.current_round,
                        'action': 'collect_figures',
                        'collected_figures': collected_figures,
                        'filtered_figures_to_collect': filtered_figures_to_collect,
                        'response': response
                    })
           
            except Exception as e:
                error_msg = f"LLMè°ƒç”¨é”™è¯¯: {str(e)}"
                print(f"âŒ {error_msg}")
                self.conversation_history.append({
                    'role': 'user',
                    'content': f"å‘ç”Ÿé”™è¯¯: {error_msg}ï¼Œè¯·é‡æ–°ç”Ÿæˆä»£ç ã€‚"
                })
        # ç”Ÿæˆæœ€ç»ˆæ€»ç»“
        if self.current_round >= max_rounds:
            print(f"\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§è½®æ•° ({max_rounds})ï¼Œåˆ†æç»“æŸ")
        
        return self._generate_final_report(self.analysis_results)


    def quick_analysis(self, query,files=None, llm_config=None, output_dir="outputs", max_rounds=10,absolute_path=False):
        """
        å¿«é€Ÿæ•°æ®åˆ†æå‡½æ•°
        
        Args:
            query: åˆ†æéœ€æ±‚ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
            files: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            max_rounds: æœ€å¤§åˆ†æè½®æ•°
            
        Returns:
            dict: åˆ†æç»“æœ
        """
        # agent = self.create_agent(llm_config=llm_config, output_dir=output_dir, max_rounds=max_rounds, absolute_path=absolute_path)
        return self.analyze(query, files or [], max_rounds=max_rounds)
    
    def get_company_infos(self, data_dir="./company_info"):
        """è·å–å…¬å¸ä¿¡æ¯"""
        all_files = os.listdir(data_dir)
        company_infos = ""
        for file in all_files:
            if file.endswith(".txt"):
                company_name = file.split(".")[0]
                with open(os.path.join(data_dir, file), 'r', encoding='utf-8') as f:
                    content = f.read()
                company_infos += f"ã€å…¬å¸ä¿¡æ¯å¼€å§‹ã€‘\nå…¬å¸åç§°: {company_name}\n{content}\nã€å…¬å¸ä¿¡æ¯ç»“æŸã€‘\n\n"
        return company_infos
    
    def get_company_files(self, data_dir):
        """è·å–å…¬å¸æ–‡ä»¶"""
        all_files = glob.glob(f"{data_dir}/*.csv")
        companies = {}
        for file in all_files:
            filename = os.path.basename(file)
            company_name = filename.split("_")[0]
            companies.setdefault(company_name, []).append(file)
        return companies
    
    def analyze_individual_company(self, company_name, files, llm_config, query=None, verbose=True):
        """åˆ†æå•ä¸ªå…¬å¸"""
        if query is None:
            query = "åŸºäºè¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰ä»·å€¼çš„å†…å®¹ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
        report = self.quick_analysis(
            query=query, files=files, llm_config=llm_config, 
            absolute_path=True, max_rounds=20
        )
        return report
    
    def format_final_reports(self, all_reports):
        """æ ¼å¼åŒ–æœ€ç»ˆæŠ¥å‘Š"""
        formatted_output = []
        for company_name, report in all_reports.items():
            formatted_output.append(f"ã€{company_name}è´¢åŠ¡æ•°æ®åˆ†æç»“æœå¼€å§‹ã€‘")
            final_report = report.get("final_report", "æœªç”ŸæˆæŠ¥å‘Š")
            formatted_output.append(final_report)
            formatted_output.append(f"ã€{company_name}è´¢åŠ¡æ•°æ®åˆ†æç»“æœç»“æŸã€‘")
            formatted_output.append("")
        return "\n".join(formatted_output)
    
    def analyze_companies_in_directory(self, data_directory, llm_config, query="åŸºäºè¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰ä»·å€¼çš„å†…å®¹ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"):
        """åˆ†æç›®å½•ä¸­çš„æ‰€æœ‰å…¬å¸"""
        company_files = self.get_company_files(data_directory)
        all_reports = {}
        for company_name, files in company_files.items():
            report = self.analyze_individual_company(company_name, files, llm_config, query, verbose=False)
            if report:
                all_reports[company_name] = report
        return all_reports
    
    def compare_two_companies(self, company1_name, company1_files, company2_name, company2_files, llm_config):
        """æ¯”è¾ƒä¸¤ä¸ªå…¬å¸"""
        query = "åŸºäºä¸¤ä¸ªå…¬å¸çš„è¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰å…±åŒç‚¹çš„éƒ¨åˆ†ï¼Œç»˜åˆ¶å¯¹æ¯”åˆ†æçš„è¡¨æ ¼ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
        all_files = company1_files + company2_files
        report = self.quick_analysis(
            query=query,
            files=all_files,
            llm_config=llm_config,
            absolute_path=True,
            max_rounds=20
        )
        return report
    
    def run_comparison_analysis(self, data_directory, target_company_name, llm_config):
        """è¿è¡Œå¯¹æ¯”åˆ†æ"""
        company_files = self.get_company_files(data_directory)
        if not company_files or target_company_name not in company_files:
            return {}
        competitors = [company for company in company_files.keys() if company != target_company_name]
        comparison_reports = {}
        for competitor in competitors:
            comparison_key = f"{target_company_name}_vs_{competitor}"
            report = self.compare_two_companies(
                target_company_name, company_files[target_company_name],
                competitor, company_files[competitor],
                llm_config
            )
            if report:
                comparison_reports[comparison_key] = {
                    'company1': target_company_name,
                    'company2': competitor,
                    'report': report
                }
        return comparison_reports
    
    def merge_reports(self, individual_reports, comparison_reports):
        """åˆå¹¶æŠ¥å‘Š"""
        merged = {}
        for company, report in individual_reports.items():
            merged[company] = report
        for comp_key, comp_data in comparison_reports.items():
            merged[comp_key] = comp_data['report']
        return merged
    
    def get_sensetime_files(self, data_dir):
        """è·å–å•†æ±¤ç§‘æŠ€çš„è´¢åŠ¡æ•°æ®æ–‡ä»¶"""
        all_files = glob.glob(f"{data_dir}/*.csv")
        sensetime_files = []
        for file in all_files:
            filename = os.path.basename(file)
            company_name = filename.split("_")[0]
            if "å•†æ±¤" in company_name or "SenseTime" in company_name:
                sensetime_files.append(file)
        return sensetime_files
    
    def analyze_sensetime_valuation(self, files, llm_config):
        """åˆ†æå•†æ±¤ç§‘æŠ€çš„ä¼°å€¼ä¸é¢„æµ‹"""
        query = "åŸºäºä¸‰å¤§è¡¨çš„æ•°æ®ï¼Œæ„å»ºä¼°å€¼ä¸é¢„æµ‹æ¨¡å‹ï¼Œæ¨¡æ‹Ÿå…³é”®å˜é‡å˜åŒ–å¯¹è´¢åŠ¡ç»“æœçš„å½±å“,å¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
        report = self.quick_analysis(
            query=query, files=files, llm_config=llm_config, absolute_path=True, max_rounds=20
        )
        return report
    
    def run_financial_analysis(self):
        """
        è¿è¡Œè´¢åŠ¡åˆ†æï¼ŒåŒ…æ‹¬å•å…¬å¸åˆ†æã€ä¸¤ä¸¤å¯¹æ¯”åˆ†æã€åˆå¹¶æŠ¥å‘Šã€ä¼°å€¼ä¸é¢„æµ‹åˆ†æã€‚
        è¿”å›ï¼šmerged_results, sensetime_valuation_report
        """
        print("\nğŸ“ˆ è¿è¡Œè´¢åŠ¡åˆ†æ...")
        # å•å…¬å¸åˆ†æ
        results = self.analyze_companies_in_directory(self.data_dir, self.llm_config)
        # ä¸¤ä¸¤å¯¹æ¯”åˆ†æ
        comparison_results = self.run_comparison_analysis(
            self.data_dir, self.target_company, self.llm_config
        )
        # åˆå¹¶æ‰€æœ‰æŠ¥å‘Š
        merged_results = self.merge_reports(results, comparison_results)
        # å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æ
        sensetime_files = self.get_sensetime_files(self.data_dir)
        sensetime_valuation_report = None
        if sensetime_files:
            sensetime_valuation_report = self.analyze_sensetime_valuation(sensetime_files, self.llm_config)
        return merged_results, sensetime_valuation_report
    
    # ========== æ·±åº¦ç ”æŠ¥ç”Ÿæˆç›¸å…³æ–¹æ³• ==========
    
    def load_report_content(self, md_path):
        """åŠ è½½æŠ¥å‘Šå†…å®¹"""
        with open(md_path, "r", encoding="utf-8") as f:
            return f.read()
    
    def get_background(self):
        """è·å–èƒŒæ™¯ä¿¡æ¯"""
        return '''
æœ¬æŠ¥å‘ŠåŸºäºè‡ªåŠ¨åŒ–é‡‡é›†ä¸åˆ†ææµç¨‹ï¼Œæ¶µç›–å¦‚ä¸‹ç¯èŠ‚ï¼š
- å…¬å¸åŸºç¡€ä¿¡æ¯ç­‰æ•°æ®å‡é€šè¿‡akshareã€å…¬å¼€å¹´æŠ¥ã€ä¸»æµè´¢ç»æ•°æ®æºè‡ªåŠ¨é‡‡é›†ã€‚
- è´¢åŠ¡ä¸‰å¤§æŠ¥è¡¨æ•°æ®æ¥æºï¼šä¸œæ–¹è´¢å¯Œ-æ¸¯è‚¡-è´¢åŠ¡æŠ¥è¡¨-ä¸‰å¤§æŠ¥è¡¨ (https://emweb.securities.eastmoney.com/PC_HKF10/FinancialAnalysis/index)
- ä¸»è¥ä¸šåŠ¡ä¿¡æ¯æ¥æºï¼šåŒèŠ±é¡º-ä¸»è¥ä»‹ç» (https://basic.10jqka.com.cn/new/000066/operate.html)
- è‚¡ä¸œç»“æ„ä¿¡æ¯æ¥æºï¼šåŒèŠ±é¡º-è‚¡ä¸œä¿¡æ¯ (https://basic.10jqka.com.cn/HK0020/holder.html) é€šè¿‡ç½‘é¡µçˆ¬è™«æŠ€æœ¯è‡ªåŠ¨é‡‡é›†
- è¡Œä¸šä¿¡æ¯é€šè¿‡DuckDuckGoç­‰å…¬å¼€æœç´¢å¼•æ“è‡ªåŠ¨æŠ“å–ï¼Œå¼•ç”¨äº†æƒå¨æ–°é—»ã€ç ”æŠ¥ã€å…¬å¸å…¬å‘Šç­‰ã€‚
- è´¢åŠ¡åˆ†æã€å¯¹æ¯”åˆ†æã€ä¼°å€¼ä¸é¢„æµ‹å‡ç”±å¤§æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰è‡ªåŠ¨ç”Ÿæˆï¼Œç»“åˆäº†è¡Œä¸šå¯¹æ ‡ã€è´¢åŠ¡æ¯”ç‡ã€æ²»ç†ç»“æ„ç­‰å¤šç»´åº¦å†…å®¹ã€‚
- ç›¸å…³æ•°æ®ä¸åˆ†æå‡åœ¨è„šæœ¬è‡ªåŠ¨åŒ–æµç¨‹ä¸‹å®Œæˆï¼Œç¡®ä¿æ•°æ®æ¥æºå¯è¿½æº¯ã€åˆ†æé€»è¾‘é€æ˜ã€‚
- è¯¦ç»†å¼•ç”¨ä¸å¤–éƒ¨é“¾æ¥å·²åœ¨æ­£æ–‡ä¸­æ ‡æ³¨ã€‚
- æ•°æ®æ¥å£è¯´æ˜ä¸å…è´£å£°æ˜è§æ–‡æœ«ã€‚
'''
    
    def generate_outline(self, llm, background, report_content):
        """ç”Ÿæˆå¤§çº²"""
        outline_prompt = f"""
ä½ æ˜¯ä¸€ä½é¡¶çº§é‡‘èåˆ†æå¸ˆå’Œç ”æŠ¥æ’°å†™ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹èƒŒæ™¯å’Œè´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½è¯¦å°½çš„ã€Šå•†æ±¤ç§‘æŠ€å…¬å¸ç ”æŠ¥ã€‹åˆ†æ®µå¤§çº²ï¼Œè¦æ±‚ï¼š
- ä»¥yamlæ ¼å¼è¾“å‡ºï¼ŒåŠ¡å¿…ç”¨```yamlå’Œ```åŒ…è£¹æ•´ä¸ªyamlå†…å®¹ï¼Œä¾¿äºåç»­è‡ªåŠ¨åˆ†å‰²ã€‚
- æ¯ä¸€é¡¹ä¸ºä¸€ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œæ¯éƒ¨åˆ†éœ€åŒ…å«ï¼š
  - part_title: ç« èŠ‚æ ‡é¢˜
  - part_desc: æœ¬éƒ¨åˆ†å†…å®¹ç®€ä»‹
- ç« èŠ‚éœ€è¦†ç›–å…¬å¸åŸºæœ¬é¢ã€è´¢åŠ¡åˆ†æã€è¡Œä¸šå¯¹æ¯”ã€ä¼°å€¼ä¸é¢„æµ‹ã€æ²»ç†ç»“æ„ã€æŠ•èµ„å»ºè®®ã€é£é™©æç¤ºã€æ•°æ®æ¥æºç­‰ã€‚
- åªè¾“å‡ºyamlæ ¼å¼çš„åˆ†æ®µå¤§çº²ï¼Œä¸è¦è¾“å‡ºæ­£æ–‡å†…å®¹ã€‚

ã€èƒŒæ™¯è¯´æ˜å¼€å§‹ã€‘
{background}
ã€èƒŒæ™¯è¯´æ˜ç»“æŸã€‘

ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹å¼€å§‹ã€‘
{report_content}
ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ç»“æŸã€‘
"""
        outline_list = llm.call(
            outline_prompt,
            system_prompt="ä½ æ˜¯ä¸€ä½é¡¶çº§é‡‘èåˆ†æå¸ˆå’Œç ”æŠ¥æ’°å†™ä¸“å®¶ï¼Œå–„äºç»“æ„åŒ–ã€åˆ†æ®µè§„åˆ’è¾“å‡ºï¼Œåˆ†æ®µå¤§çº²å¿…é¡»ç”¨```yamlåŒ…è£¹ï¼Œä¾¿äºåç»­è‡ªåŠ¨åˆ†å‰²ã€‚",
            max_tokens=4096,
            temperature=0.3
        )
        print("\n===== ç”Ÿæˆçš„åˆ†æ®µå¤§çº²å¦‚ä¸‹ =====\n")
        print(outline_list)
        try:
            if '```yaml' in outline_list:
                yaml_block = outline_list.split('```yaml')[1].split('```')[0]
            else:
                yaml_block = outline_list
            parts = yaml.safe_load(yaml_block)
            if isinstance(parts, dict):
                parts = list(parts.values())
        except Exception as e:
            print(f"[å¤§çº²yamlè§£æå¤±è´¥] {e}")
            parts = []
        return parts
    
    def generate_section(self, llm, part_title, prev_content, background, report_content, is_last):
        """ç”Ÿæˆç« èŠ‚"""
        section_prompt = f"""
ä½ æ˜¯ä¸€ä½é¡¶çº§é‡‘èåˆ†æå¸ˆå’Œç ”æŠ¥æ’°å†™ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹å†…å®¹ï¼Œç›´æ¥è¾“å‡º\"{part_title}\"è¿™ä¸€éƒ¨åˆ†çš„å®Œæ•´ç ”æŠ¥å†…å®¹ã€‚

**é‡è¦è¦æ±‚ï¼š**
1. ç›´æ¥è¾“å‡ºå®Œæ•´å¯ç”¨çš„ç ”æŠ¥å†…å®¹ï¼Œä»¥\"## {part_title}\"å¼€å¤´
2. åœ¨æ­£æ–‡ä¸­å¼•ç”¨æ•°æ®ã€äº‹å®ã€å›¾ç‰‡ç­‰ä¿¡æ¯æ—¶ï¼Œé€‚å½“ä½ç½®æ’å…¥å‚è€ƒèµ„æ–™ç¬¦å·ï¼ˆå¦‚[1][2][3]ï¼‰ï¼Œç¬¦å·éœ€ä¸æ–‡æœ«å¼•ç”¨æ–‡çŒ®ç¼–å·ä¸€è‡´
3. **å›¾ç‰‡å¼•ç”¨è¦æ±‚ï¼ˆåŠ¡å¿…ä¸¥æ ¼éµå®ˆï¼‰ï¼š**
   - åªå…è®¸å¼•ç”¨ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­çœŸå®å­˜åœ¨çš„å›¾ç‰‡åœ°å€ï¼ˆæ ¼å¼å¦‚ï¼š./images/å›¾ç‰‡åå­—.pngï¼‰ï¼Œå¿…é¡»ä¸åŸæ–‡å®Œå…¨ä¸€è‡´ã€‚
   - ç¦æ­¢è™šæ„ã€æœæ’°ã€æ”¹ç¼–ã€çŒœæµ‹å›¾ç‰‡åœ°å€ï¼Œæœªåœ¨ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­å‡ºç°çš„å›¾ç‰‡ä¸€å¾‹ä¸å¾—å¼•ç”¨ã€‚
   - å¦‚éœ€æ’å…¥å›¾ç‰‡ï¼Œå¿…é¡»å…ˆåœ¨ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­æŸ¥æ‰¾ï¼Œæœªæ‰¾åˆ°åˆ™ä¸æ’å…¥å›¾ç‰‡ï¼Œç»ä¸ç¼–é€ å›¾ç‰‡ã€‚
   - å¦‚å¼•ç”¨äº†ä¸å­˜åœ¨çš„å›¾ç‰‡ï¼Œå°†è¢«åˆ¤ä¸ºé”™è¯¯è¾“å‡ºã€‚
4. ä¸è¦è¾“å‡ºä»»ä½•ã€xxxå¼€å§‹ã€‘ã€xxxç»“æŸã€‘ç­‰åˆ†éš”ç¬¦
5. ä¸è¦è¾“å‡º\"å»ºè®®è¡¥å……\"ã€\"éœ€è¦æ·»åŠ \"ç­‰æç¤ºæ€§è¯­è¨€
6. ä¸è¦ç¼–é€ å›¾ç‰‡åœ°å€æˆ–æ•°æ®
7. å†…å®¹è¦è¯¦å®ã€ä¸“ä¸šï¼Œå¯ç›´æ¥ä½¿ç”¨

**æ•°æ®æ¥æºæ ‡æ³¨ï¼š**
- è´¢åŠ¡æ•°æ®æ ‡æ³¨ï¼šï¼ˆæ•°æ®æ¥æºï¼šä¸œæ–¹è´¢å¯Œ-æ¸¯è‚¡-è´¢åŠ¡æŠ¥è¡¨[1]ï¼‰
- ä¸»è¥ä¸šåŠ¡ä¿¡æ¯æ ‡æ³¨ï¼šï¼ˆæ•°æ®æ¥æºï¼šåŒèŠ±é¡º-ä¸»è¥ä»‹ç»[2]ï¼‰
- è‚¡ä¸œç»“æ„ä¿¡æ¯æ ‡æ³¨ï¼šï¼ˆæ•°æ®æ¥æºï¼šåŒèŠ±é¡º-è‚¡ä¸œä¿¡æ¯ç½‘é¡µçˆ¬è™«[3]ï¼‰

ã€æœ¬æ¬¡ä»»åŠ¡ã€‘
{part_title}

ã€å·²ç”Ÿæˆå‰æ–‡ã€‘
{prev_content}

ã€èƒŒæ™¯è¯´æ˜å¼€å§‹ã€‘
{background}
ã€èƒŒæ™¯è¯´æ˜ç»“æŸã€‘

ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹å¼€å§‹ã€‘
{report_content}
ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ç»“æŸã€‘
"""
        if is_last:
            section_prompt += """
è¯·åœ¨æœ¬èŠ‚æœ€åä»¥"å¼•ç”¨æ–‡çŒ®"æ ¼å¼ï¼Œåˆ—å‡ºæ‰€æœ‰æ­£æ–‡ä¸­ç”¨åˆ°çš„å‚è€ƒèµ„æ–™ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[1] ä¸œæ–¹è´¢å¯Œ-æ¸¯è‚¡-è´¢åŠ¡æŠ¥è¡¨: https://emweb.securities.eastmoney.com/PC_HKF10/FinancialAnalysis/index
[2] åŒèŠ±é¡º-ä¸»è¥ä»‹ç»: https://basic.10jqka.com.cn/new/000066/operate.html
[3] åŒèŠ±é¡º-è‚¡ä¸œä¿¡æ¯: https://basic.10jqka.com.cn/HK0020/holder.html
"""
        section_text = llm.call(
            section_prompt,
            system_prompt="ä½ æ˜¯é¡¶çº§é‡‘èåˆ†æå¸ˆï¼Œä¸“é—¨ç”Ÿæˆå®Œæ•´å¯ç”¨çš„ç ”æŠ¥å†…å®¹ã€‚è¾“å‡ºå¿…é¡»æ˜¯å®Œæ•´çš„ç ”æŠ¥æ­£æ–‡ï¼Œæ— éœ€ç”¨æˆ·ä¿®æ”¹ã€‚ä¸¥æ ¼ç¦æ­¢è¾“å‡ºåˆ†éš”ç¬¦ã€å»ºè®®æ€§è¯­è¨€æˆ–è™šæ„å†…å®¹ã€‚åªå…è®¸å¼•ç”¨çœŸå®å­˜åœ¨äºã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­çš„å›¾ç‰‡åœ°å€ï¼Œä¸¥ç¦è™šæ„ã€çŒœæµ‹ã€æ”¹ç¼–å›¾ç‰‡è·¯å¾„ã€‚å¦‚å¼•ç”¨äº†ä¸å­˜åœ¨çš„å›¾ç‰‡ï¼Œå°†è¢«åˆ¤ä¸ºé”™è¯¯è¾“å‡ºã€‚",
            max_tokens=8192,
            temperature=0.5
        )
        return section_text
    
    def save_markdown(self, content, output_file):
        """ä¿å­˜markdownæ–‡ä»¶"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"\nğŸ“ æ·±åº¦è´¢åŠ¡ç ”æŠ¥åˆ†æå·²ä¿å­˜åˆ°: {output_file}")
    
    def format_markdown(self, output_file):
        """æ ¼å¼åŒ–markdownæ–‡ä»¶"""
        try:
            import subprocess
            format_cmd = ["mdformat", output_file]
            subprocess.run(format_cmd, check=True, capture_output=True, text=True, encoding='utf-8')
            print(f"âœ… å·²ç”¨ mdformat æ ¼å¼åŒ– Markdown æ–‡ä»¶: {output_file}")
        except Exception as e:
            print(f"[æç¤º] mdformat æ ¼å¼åŒ–å¤±è´¥: {e}\nè¯·ç¡®ä¿å·²å®‰è£… mdformat (pip install mdformat)")
    
    def convert_to_docx(self, output_file, docx_output=None):
        """è½¬æ¢ä¸ºWordæ–‡æ¡£"""
        if docx_output is None:
            docx_output = output_file.replace('.md', '.docx')
        try:
            import subprocess
            import os
            pandoc_cmd = [
                "pandoc",
                output_file,
                "-o",
                docx_output,
                "--standalone",
                "--resource-path=.",
                "--extract-media=."
            ]
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            subprocess.run(pandoc_cmd, check=True, capture_output=True, text=True, encoding='utf-8', env=env)
            print(f"\nğŸ“„ Wordç‰ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {docx_output}")
        except subprocess.CalledProcessError as e:
            print(f"[æç¤º] pandocè½¬æ¢å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {e.stderr}")
            print("[å»ºè®®] æ£€æŸ¥å›¾ç‰‡è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨ --extract-media é€‰é¡¹")
        except Exception as e:
            print(f"[æç¤º] è‹¥éœ€ç”ŸæˆWordæ–‡æ¡£ï¼Œè¯·ç¡®ä¿å·²å®‰è£…pandocã€‚å½“å‰è½¬æ¢å¤±è´¥: {e}")
    
    # ========== å›¾ç‰‡å¤„ç†ç›¸å…³æ–¹æ³• ==========
    
    def ensure_dir(self, path):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        if not os.path.exists(path):
            os.makedirs(path)
    
    def is_url(self, path):
        """åˆ¤æ–­æ˜¯å¦ä¸ºURL"""
        return path.startswith('http://') or path.startswith('https://')
    
    def download_image(self, url, save_path):
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            resp = requests.get(url, stream=True, timeout=10)
            resp.raise_for_status()
            with open(save_path, 'wb') as f:
                for chunk in resp.iter_content(1024):
                    f.write(chunk)
            return True
        except Exception as e:
            print(f"[ä¸‹è½½å¤±è´¥] {url}: {e}")
            return False
    
    def copy_image(self, src, dst):
        """å¤åˆ¶å›¾ç‰‡"""
        try:
            shutil.copy2(src, dst)
            return True
        except Exception as e:
            print(f"[å¤åˆ¶å¤±è´¥] {src}: {e}")
            return False
    
    def extract_images_from_markdown(self, md_path, images_dir, new_md_path):
        """ä»markdownä¸­æå–å›¾ç‰‡"""
        self.ensure_dir(images_dir)
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # åŒ¹é… ![alt](path) å½¢å¼çš„å›¾ç‰‡
        pattern = re.compile(r'!\[[^\]]*\]\(([^)]+)\)')
        matches = pattern.findall(content)
        used_names = set()
        replace_map = {}
        not_exist_set = set()

        for img_path in matches:
            img_path = img_path.strip()
            # å–æ–‡ä»¶å
            if self.is_url(img_path):
                filename = os.path.basename(urlparse(img_path).path)
            else:
                filename = os.path.basename(img_path)
            # é˜²æ­¢é‡å
            base, ext = os.path.splitext(filename)
            i = 1
            new_filename = filename
            while new_filename in used_names:
                new_filename = f"{base}_{i}{ext}"
                i += 1
            used_names.add(new_filename)
            new_img_path = os.path.join(images_dir, new_filename)
            # ä¸‹è½½æˆ–å¤åˆ¶
            img_exists = True
            if self.is_url(img_path):
                success = self.download_image(img_path, new_img_path)
                if not success:
                    img_exists = False
            else:
                # æ”¯æŒç»å¯¹å’Œç›¸å¯¹è·¯å¾„
                abs_img_path = img_path
                if not os.path.isabs(img_path):
                    abs_img_path = os.path.join(os.path.dirname(md_path), img_path)
                if not os.path.exists(abs_img_path):
                    print(f"[è­¦å‘Š] æœ¬åœ°å›¾ç‰‡ä¸å­˜åœ¨: {abs_img_path}")
                    img_exists = False
                else:
                    self.copy_image(abs_img_path, new_img_path)
            # è®°å½•æ›¿æ¢
            if img_exists:
                replace_map[img_path] = f'./images/{new_filename}'
            else:
                not_exist_set.add(img_path)

        # æ›¿æ¢ markdown å†…å®¹ï¼Œä¸å­˜åœ¨çš„å›¾ç‰‡ç›´æ¥åˆ é™¤æ•´ä¸ªå›¾ç‰‡è¯­æ³•
        def replace_func(match):
            orig = match.group(1).strip()
            if orig in not_exist_set:
                return ''  # åˆ é™¤ä¸å­˜åœ¨çš„å›¾ç‰‡è¯­æ³•
            return match.group(0).replace(orig, replace_map.get(orig, orig))

        new_content = pattern.sub(replace_func, content)
        with open(new_md_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        print(f"å›¾ç‰‡å¤„ç†å®Œæˆï¼æ–°æ–‡ä»¶: {new_md_path}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
    parser = argparse.ArgumentParser(description='æ•´åˆçš„é‡‘èç ”æŠ¥ç”Ÿæˆå™¨')
    parser.add_argument('--search-engine', choices=['ddg', 'sogou'], default='sogou',
                       help='æœç´¢å¼•æ“é€‰æ‹©: ddg (DuckDuckGo) æˆ– sogou (æœç‹—), é»˜è®¤: ddg')
    parser.add_argument('--company', default='å•†æ±¤ç§‘æŠ€', help='ç›®æ ‡å…¬å¸åç§°')
    parser.add_argument('--code', default='00020', help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--market', default='HK', help='å¸‚åœºä»£ç ')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
    generator = IntegratedResearchReportGenerator(
        target_company=args.company,
        target_company_code=args.code, 
        target_company_market=args.market,
        search_engine=args.search_engine
    )
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    basic_report, deep_report = generator.run_full_pipeline()
    
    print("\n" + "="*100)
    print("ğŸ¯ ç¨‹åºæ‰§è¡Œå®Œæ¯•ï¼ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
    print(f"ğŸ“Š åŸºç¡€åˆ†ææŠ¥å‘Š: {basic_report}")
    print(f"ğŸ“‹ æ·±åº¦ç ”æŠ¥: {deep_report}")
    print("="*100)


if __name__ == "__main__":
    main()
